{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d8c18729",
      "metadata": {
        "id": "d8c18729"
      },
      "source": [
        "## 12-7. Project: 멋진 챗봇 만들기\n",
        "**라이브러리 버전을 확인해 봅니다**\n",
        "\n",
        "---\n",
        "사용할 라이브러리 버전을 둘러봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45da39b",
      "metadata": {
        "id": "e45da39b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "\n",
        "import gensim\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f8288f",
      "metadata": {
        "id": "73f8288f"
      },
      "source": [
        "지난 노드에서 **챗봇과 번역기는 같은 집안**이라고 했던 말을 기억하시나요?\n",
        "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해 봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해 보세요!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a2cd8f",
      "metadata": {
        "id": "d5a2cd8f"
      },
      "source": [
        "Step 1. 데이터 다운로드\n",
        "\n",
        "---\n",
        "준비하기 단계에서 심볼릭 링크를 생성했다면 아래 파일이 ```ChatbotData .csv```라는 이름으로 저장되어 있을거예요. ```csv``` 파일을 읽는 데에는 ```pandas``` 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 ```questions```, ```answers``` 변수에 나눠서 저장하세요!\n",
        "\n",
        "- [songys/Chatbot_data](https://github.com/songys/Chatbot_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be29f40f",
      "metadata": {
        "id": "be29f40f",
        "outputId": "73183330-459b-4d0f-8147-d92d748e1728"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('~/aiffel/transformer_chatbot/data/ChatbotData .csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d808565f",
      "metadata": {
        "id": "d808565f"
      },
      "outputs": [],
      "source": [
        "questions = data['Q']\n",
        "answers = data['A']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256b7248",
      "metadata": {
        "id": "256b7248",
        "outputId": "41620822-b269-46d5-8e67-6a4a5cc84ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11823 11823\n",
            "--------------------------\n",
            "0             12시 땡!\n",
            "1        1지망 학교 떨어졌어\n",
            "2       3박4일 놀러가고 싶다\n",
            "3    3박4일 정도 놀러가고 싶다\n",
            "4            PPL 심하네\n",
            "Name: Q, dtype: object\n",
            "--------------------------\n",
            "0     하루가 또 가네요.\n",
            "1      위로해 드립니다.\n",
            "2    여행은 언제나 좋죠.\n",
            "3    여행은 언제나 좋죠.\n",
            "4     눈살이 찌푸려지죠.\n",
            "Name: A, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(len(questions), len(answers))\n",
        "print('--------------------------')\n",
        "print(questions.head())\n",
        "print('--------------------------')\n",
        "print(answers.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb36cae2",
      "metadata": {
        "id": "eb36cae2"
      },
      "source": [
        "Step 2. 데이터 정제\n",
        "\n",
        "---\n",
        "아래 조건을 만족하는 ```preprocess_sentence()``` 함수를 구현하세요.\n",
        "\n",
        "1. 영문자의 경우, **모두 소문자로 변환**합니다.\n",
        "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 **정규식을 활용하여 모두 제거**합니다.\n",
        "\n",
        "*문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88717555",
      "metadata": {
        "id": "88717555"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9?.!,]+\", \" \", sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969baea1",
      "metadata": {
        "id": "969baea1"
      },
      "source": [
        "Step 3. 데이터 토큰화\n",
        "\n",
        "---\n",
        "토큰화에는 *KoNLPy*의 ```mecab``` 클래스를 사용합니다.\n",
        "\n",
        "아래 조건을 만족하는 ```build_corpus()``` 함수를 구현하세요!\n",
        "\n",
        "1. **소스 문장 데이터**와 **타겟 문장 데이터**를 입력으로 받습니다.\n",
        "2. 데이터를 앞서 정의한 **```preprocess_sentence()```** 함수로 **정제하고, 토큰화**합니다.\n",
        "3. 토큰화는 **전달받은 토크나이즈 함수를 사용**합니다. 이번엔 ```**mecab.morphs**``` 함수를 전달하시면 됩니다.\n",
        "4. 토큰의 개수가 일정 길이 이상인 문장은 **데이터에서 제외**합니다.\n",
        "5. **중복되는 문장은 데이터에서 제외**합니다. ```소스 : 타겟``` 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
        "\n",
        "구현한 함수를 활용하여 ```questions``` 와 ```answers``` 를 각각 ```que_corpus``` , ```ans_corpus``` 에 토큰화하여 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b14843",
      "metadata": {
        "id": "27b14843"
      },
      "outputs": [],
      "source": [
        "mecab = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b6ff92",
      "metadata": {
        "id": "e4b6ff92"
      },
      "outputs": [],
      "source": [
        "def build_corpus(src_data, tgt_data):\n",
        "    mecab_src_corpus, mecab_tgt_corpus = [], []\n",
        "    mecab_src_len_list, mecab_tgt_len_list = [], []\n",
        "    \n",
        "    for s, t in zip(src_data, tgt_data):\n",
        "        s = mecab.morphs(preprocess_sentence(s))\n",
        "        t = mecab.morphs(preprocess_sentence(t))\n",
        "        \n",
        "        mecab_src_corpus.append(s)\n",
        "        mecab_tgt_corpus.append(t)\n",
        "        \n",
        "        mecab_src_len_list.append(len(s))\n",
        "        mecab_tgt_len_list.append(len(t))\n",
        "\n",
        "    mecab_num_tokens = mecab_src_len_list + mecab_tgt_len_list\n",
        "    \n",
        "    mean_len = np.mean(mecab_num_tokens)\n",
        "    max_len = np.max(mecab_num_tokens)\n",
        "    mid_len = np.median([mean_len, max_len])\n",
        "    print(f'mid_len : {mid_len}')\n",
        "    \n",
        "    src_corpus, tgt_corpus = [], []\n",
        "    for q, a in zip(mecab_src_corpus, mecab_tgt_corpus):\n",
        "        if len(q) <= mid_len and len(a) <= mid_len:\n",
        "            if q not in src_corpus and a not in tgt_corpus:\n",
        "                src_corpus.append(q)\n",
        "                tgt_corpus.append(a)\n",
        "    \n",
        "    return src_corpus, tgt_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6497f393",
      "metadata": {
        "id": "6497f393",
        "outputId": "1791d37b-9b5b-4b5c-dd26-ad2318f833ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mid_len : 23.849149961938593\n"
          ]
        }
      ],
      "source": [
        "que_corpus, ans_corpus = build_corpus(questions, answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c8992b",
      "metadata": {
        "id": "f4c8992b",
        "outputId": "ff98f4b2-702a-45ef-c277-4b57d8a6235d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['12', '시', '땡', '!'],\n",
              " ['1', '지망', '학교', '떨어졌', '어'],\n",
              " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
              " ['ppl', '심하', '네'],\n",
              " ['sd', '카드', '망가졌', '어']]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "que_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6508689",
      "metadata": {
        "id": "a6508689",
        "outputId": "05f03416-b37c-4a56-90dc-db604dafe52d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['하루', '가', '또', '가', '네요', '.'],\n",
              " ['위로', '해', '드립니다', '.'],\n",
              " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
              " ['눈살', '이', '찌푸려', '지', '죠', '.'],\n",
              " ['다시', '새로', '사', '는', '게', '마음', '편해요', '.']]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b2b3b1",
      "metadata": {
        "id": "01b2b3b1",
        "outputId": "625e5341-65d1-45ab-d261-bd36bddbd95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7637\n",
            "7637\n"
          ]
        }
      ],
      "source": [
        "print(len(que_corpus))\n",
        "print(len(ans_corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b971d6",
      "metadata": {
        "id": "80b971d6"
      },
      "source": [
        "Step 4. Augmentation\n",
        "\n",
        "---\n",
        "우리에게 주어진 데이터는 **1만 개가량으로 적은 편**에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해 봐야겠죠? **Lexical Substitution을 실제로 적용**해 보도록 하겠습니다.\n",
        "\n",
        "아래 링크를 참고하여 **한국어로 사전 훈련된 Embedding 모델을 다운로드**합니다. ```Korean (w)``` 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 ```Korean (w)```를 찾아 다운로드하고, ```ko.bin``` 파일을 얻으세요!\n",
        "\n",
        "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
        "\n",
        "다운로드한 모델을 활용해 **데이터를 Augmentation** 하세요! 앞서 정의한 ```lexical_sub()``` 함수를 참고하면 도움이 많이 될 겁니다.\n",
        "\n",
        "*Augmentation*된 ```*que_corpus*``` 와 원본 ```*ans_corpus*``` 가 병렬을 이루도록, 이후엔 반대로 원본 ```*que_corpus*``` 와 Augmentation된 ```*ans_corpus*``` 가 병렬을 이루도록 하여 **전체 데이터가 원래의 3배가량으로 늘어나도록** 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355a3578",
      "metadata": {
        "id": "355a3578",
        "outputId": "ac69d9c4-d44d-4636-c2c7-2f9cbedd8035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==3.8.3 in /opt/conda/lib/python3.9/site-packages (3.8.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim==3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5ffff7",
      "metadata": {
        "id": "6a5ffff7"
      },
      "outputs": [],
      "source": [
        "word2vec_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ko.bin'\n",
        "word2vec = gensim.models.Word2Vec.load(word2vec_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7714789",
      "metadata": {
        "id": "d7714789"
      },
      "outputs": [],
      "source": [
        "wv = gensim.models.Word2Vec.load('./data/ko.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fe9650",
      "metadata": {
        "id": "b2fe9650"
      },
      "outputs": [],
      "source": [
        "def lexical_sub(sentence, word2vec):\n",
        "    import random\n",
        "\n",
        "    res = \"\"\n",
        "    toks = sentence\n",
        "\n",
        "    try:\n",
        "        _from = random.choice(toks)\n",
        "        _to = word2vec.most_similar(_from)[0][0]\n",
        "\n",
        "    except:   # 단어장에 없는 단어\n",
        "        return None\n",
        "\n",
        "    for tok in toks:\n",
        "        if tok is _from: res += _to + \" \"\n",
        "        else: res += tok + \" \"\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795eac30",
      "metadata": {
        "id": "795eac30",
        "outputId": "00475e00-c946-43e9-c56f-464563f45d27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/663423007.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'12 시 땡 캐치 '"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lexical_sub(que_corpus[0], wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3454119",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ff7a213c52f94e03b2d405d698301999",
            "ca16f08b2f354d92b10bf92e879c8138"
          ]
        },
        "id": "e3454119",
        "outputId": "72685e85-d62c-497e-df9b-867717724d01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/645759352.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for idx in tqdm_notebook(range(len(que_corpus))):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff7a213c52f94e03b2d405d698301999",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7637 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/663423007.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n",
            "/tmp/ipykernel_398/645759352.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for idx in tqdm_notebook(range(len(ans_corpus))):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca16f08b2f354d92b10bf92e879c8138",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7637 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "new_que_corpus = []\n",
        "new_ans_corpus = []\n",
        "\n",
        "for idx in tqdm_notebook(range(len(que_corpus))):\n",
        "    que_augmented = lexical_sub(que_corpus[idx], wv)\n",
        "    ans = ans_corpus[idx]\n",
        "    \n",
        "    if que_augmented is not None:\n",
        "        new_que_corpus.append(que_augmented.split())\n",
        "        new_ans_corpus.append(ans)\n",
        "    else:continue\n",
        "    \n",
        "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
        "    que = que_corpus[idx]\n",
        "    ans_augmented = lexical_sub(ans_corpus[idx], wv)\n",
        "    \n",
        "    if ans_augmented is not None:\n",
        "        new_que_corpus.append(que)\n",
        "        new_ans_corpus.append(ans_augmented.split())\n",
        "        \n",
        "    else:continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0334b30f",
      "metadata": {
        "id": "0334b30f",
        "outputId": "2f4a8d4e-72e6-4b35-fdbb-918db88d2996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13279\n",
            "13279\n"
          ]
        }
      ],
      "source": [
        "print(len(new_que_corpus))\n",
        "print(len(new_ans_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22d7c89",
      "metadata": {
        "id": "e22d7c89",
        "outputId": "ca769c16-672a-48df-f5b2-e90e113c932c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['12', '시', '땡', '캐치'],\n",
              " ['3', '박', '4', '일', '놀', '러', '놀드', '고', '싶', '다'],\n",
              " ['ppl', '심하', '카나'],\n",
              " ['sns', '맞', '나르', '왜', '안', '하', '지'],\n",
              " ['sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '시키', '는', '중']]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_que_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9555848",
      "metadata": {
        "id": "e9555848",
        "outputId": "ca034e5c-7659-4b57-d17c-9a51945c9c80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['하루', '가', '또', '가', '네요', '.'],\n",
              " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
              " ['눈살', '이', '찌푸려', '지', '죠', '.'],\n",
              " ['잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.'],\n",
              " ['시간', '을', '정하', '고', '해', '보', '세요', '.']]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_ans_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33941370",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9c9d5cccf6e6424a88a9de58b162e701",
            "5f072c4c1bca406fabbb89d413dcbb0d"
          ]
        },
        "id": "33941370",
        "outputId": "3d8ec17b-4a40-40fa-dec3-da7c2897effe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/4156428745.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for idx in tqdm_notebook(range(len(ans_corpus))):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c9d5cccf6e6424a88a9de58b162e701",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7637 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/663423007.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  _to = word2vec.most_similar(_from)[0][0]\n",
            "/tmp/ipykernel_398/4156428745.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for idx in tqdm_notebook(range(len(que_corpus))):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f072c4c1bca406fabbb89d413dcbb0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7637 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "new_que_new_corpus = []\n",
        "new_ans_new_corpus = []\n",
        "    \n",
        "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
        "    que = que_corpus[idx]\n",
        "    ans_augmented = lexical_sub(ans_corpus[idx], wv)\n",
        "    \n",
        "    if ans_augmented is not None:\n",
        "        new_que_new_corpus.append(que)\n",
        "        new_ans_new_corpus.append(ans_augmented.split())\n",
        "        \n",
        "    else:continue\n",
        "        \n",
        "        \n",
        "for idx in tqdm_notebook(range(len(que_corpus))):\n",
        "    que_augmented = lexical_sub(que_corpus[idx], wv)\n",
        "    ans = ans_corpus[idx]\n",
        "    \n",
        "    if que_augmented is not None:\n",
        "        new_que_new_corpus.append(que_augmented.split())\n",
        "        new_ans_new_corpus.append(ans)\n",
        "    else:continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b050866",
      "metadata": {
        "id": "4b050866",
        "outputId": "cb3e796f-3638-4a4d-b309-2e507a2a9952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13230\n",
            "13230\n"
          ]
        }
      ],
      "source": [
        "print(len(new_que_new_corpus))\n",
        "print(len(new_ans_new_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23be8782",
      "metadata": {
        "id": "23be8782",
        "outputId": "98ab62fb-77a3-4de6-ab23-08064080677d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['12', '시', '땡', '!'],\n",
              " ['1', '지망', '학교', '떨어졌', '어'],\n",
              " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
              " ['sd', '카드', '망가졌', '어'],\n",
              " ['sns', '맞', '팔', '왜', '안', '하', '지']]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_que_new_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c757159f",
      "metadata": {
        "id": "c757159f",
        "outputId": "92ed740f-0dda-44d8-9529-b8d5490a863e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['하루', '가', '또', '가', '네요', '는데'],\n",
              " ['무릎', '해', '드립니다', '.'],\n",
              " ['항해', '은', '언제나', '좋', '죠', '.'],\n",
              " ['다시', '새로', '사', '는', '게', '마음', '편해요', '는데'],\n",
              " ['잘', '몰르', '고', '있', '을', '수', '도', '있', '어요', '.']]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_ans_new_corpus[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5ae8e3",
      "metadata": {
        "id": "df5ae8e3"
      },
      "source": [
        "Step 5. 데이터 벡터화\n",
        "\n",
        "---\n",
        "타겟 데이터인 ```ans_corpus``` 에 ```<start>``` 토큰과 ```<end>``` 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ```ans_corpus``` 는 ```list``` 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8e4a86",
      "metadata": {
        "id": "fe8e4a86",
        "outputId": "5b1fd202-0d77-44c9-f0e7-4ce52937b24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<start>', '12', '시', '땡', '!', '<end>']\n"
          ]
        }
      ],
      "source": [
        "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
        "\n",
        "print([\"<start>\"] + sample_data + [\"<end>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6585b4b0",
      "metadata": {
        "id": "6585b4b0"
      },
      "source": [
        "1. 위 소스를 참고하여 타겟 데이터 전체에 ```<start>``` 토큰과 ```<end>``` 토큰을 추가해 주세요!\n",
        "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 **소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것**이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다.\n",
        "\n",
        "2. 특수 토큰을 더함으로써 ```ans_corpus``` 또한 완성이 되었으니, ```que_corpus``` 와 결합하여 **전체 데이터에 대한 단어 사전을 구축**하고 **벡터화하여 ```enc_train``` 과 ```dec_train```** 을 얻으세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51b42af",
      "metadata": {
        "id": "e51b42af"
      },
      "outputs": [],
      "source": [
        "temp = []\n",
        "    \n",
        "for corpus in new_ans_new_corpus:\n",
        "    temp.append([\"<start>\"] + corpus + [\"<end>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc940e77",
      "metadata": {
        "id": "dc940e77"
      },
      "outputs": [],
      "source": [
        "ans_corpus = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc8df25",
      "metadata": {
        "id": "9cc8df25",
        "outputId": "4a1eb970-a394-4ec2-b3fe-4a4e7123b845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13230"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ans_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568164ec",
      "metadata": {
        "id": "568164ec",
        "outputId": "bf7c1b94-494e-4c67-f2e8-44771ee45e64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['<start>', '하루', '가', '또', '가', '네요', '는데', '<end>'],\n",
              " ['<start>', '무릎', '해', '드립니다', '.', '<end>'],\n",
              " ['<start>', '항해', '은', '언제나', '좋', '죠', '.', '<end>'],\n",
              " ['<start>', '다시', '새로', '사', '는', '게', '마음', '편해요', '는데', '<end>'],\n",
              " ['<start>', '잘', '몰르', '고', '있', '을', '수', '도', '있', '어요', '.', '<end>']]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7083ea74",
      "metadata": {
        "id": "7083ea74",
        "outputId": "6fe31d3d-fd3d-472a-e3e6-92bcf4cdf9d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26460"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_data = new_que_new_corpus + ans_corpus\n",
        "len(total_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e3dceb",
      "metadata": {
        "id": "51e3dceb"
      },
      "outputs": [],
      "source": [
        "words = np.concatenate(total_data).tolist()\n",
        "counter = Counter(words)\n",
        "counter = counter.most_common(30000-2)\n",
        "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
        "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
        "index_to_word = {index:word for word, index in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63a4914",
      "metadata": {
        "id": "f63a4914",
        "outputId": "13e4688b-2ff7-4087-bade-3712cef41968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<unk>': 1,\n",
              " '.': 2,\n",
              " '<start>': 3,\n",
              " '<end>': 4,\n",
              " '이': 5,\n",
              " '는': 6,\n",
              " '하': 7,\n",
              " '을': 8,\n",
              " '가': 9,\n",
              " '좋': 10,\n",
              " '고': 11,\n",
              " '세요': 12,\n",
              " '어': 13,\n",
              " '있': 14,\n",
              " '거': 15,\n",
              " '은': 16,\n",
              " '해': 17,\n",
              " '는데': 18,\n",
              " '보': 19,\n",
              " '지': 20,\n",
              " '?': 21,\n",
              " '아': 22,\n",
              " '나': 23,\n",
              " '도': 24,\n",
              " '게': 25,\n",
              " '겠': 26,\n",
              " '사람': 27,\n",
              " '에': 28,\n",
              " '를': 29,\n",
              " '예요': 30,\n",
              " '사랑': 31,\n",
              " '어요': 32,\n",
              " '같': 33,\n",
              " '죠': 34,\n",
              " '한': 35,\n",
              " '것': 36,\n",
              " '없': 37,\n",
              " '싶': 38,\n",
              " '면': 39,\n",
              " '수': 40,\n",
              " '네요': 41,\n",
              " '다': 42,\n",
              " '의': 43,\n",
              " '네': 44,\n",
              " '안': 45,\n",
              " '친구': 46,\n",
              " '생각': 47,\n",
              " '않': 48,\n",
              " '봐요': 49,\n",
              " '아요': 50,\n",
              " '마음': 51,\n",
              " '말': 52,\n",
              " '할': 53,\n",
              " '되': 54,\n",
              " '주': 55,\n",
              " '잘': 56,\n",
              " '너무': 57,\n",
              " '남자': 58,\n",
              " '했': 59,\n",
              " '이별': 60,\n",
              " '더': 61,\n",
              " '기': 62,\n",
              " '었': 63,\n",
              " '내': 64,\n",
              " '연락': 65,\n",
              " '만': 66,\n",
              " '여자': 67,\n",
              " '일': 68,\n",
              " '들': 69,\n",
              " '힘들': 70,\n",
              " '남': 71,\n",
              " '해요': 72,\n",
              " '썸': 73,\n",
              " '그러': 74,\n",
              " '많이': 75,\n",
              " '시간': 76,\n",
              " '짝': 77,\n",
              " '으로': 78,\n",
              " '괜찮': 79,\n",
              " '한테': 80,\n",
              " '길': 81,\n",
              " '았': 82,\n",
              " '에서': 83,\n",
              " '으면': 84,\n",
              " '에요': 85,\n",
              " '건': 86,\n",
              " '때': 87,\n",
              " '에게': 88,\n",
              " '야': 89,\n",
              " '그': 90,\n",
              " '만나': 91,\n",
              " '알': 92,\n",
              " '시키': 93,\n",
              " '던': 94,\n",
              " '은데': 95,\n",
              " '요': 96,\n",
              " '좀': 97,\n",
              " '을까': 98,\n",
              " '많': 99,\n",
              " '애': 100,\n",
              " '받': 101,\n",
              " '저': 102,\n",
              " '로': 103,\n",
              " '인': 104,\n",
              " 'ㄴ다는': 105,\n",
              " '뭐': 106,\n",
              " '적': 107,\n",
              " '연애': 108,\n",
              " '먹': 109,\n",
              " '어서': 110,\n",
              " '습니다': 111,\n",
              " '못': 112,\n",
              " '자신': 113,\n",
              " '오늘': 114,\n",
              " '이제': 115,\n",
              " '!': 116,\n",
              " '잊': 117,\n",
              " '해도': 118,\n",
              " '마세요': 119,\n",
              " '모르': 120,\n",
              " '아니': 121,\n",
              " '타': 122,\n",
              " '헤어지': 123,\n",
              " '당신': 124,\n",
              " '할까': 125,\n",
              " '걸': 126,\n",
              " '라고': 127,\n",
              " '필요': 128,\n",
              " '년': 129,\n",
              " '다시': 130,\n",
              " '지만': 131,\n",
              " '끝': 132,\n",
              " '다른': 133,\n",
              " '살': 134,\n",
              " '놀드': 135,\n",
              " '전': 136,\n",
              " '방법': 137,\n",
              " '해야': 138,\n",
              " '데': 139,\n",
              " '랑': 140,\n",
              " '어떻게': 141,\n",
              " '왜': 142,\n",
              " '과': 143,\n",
              " '정말': 144,\n",
              " '지요': 145,\n",
              " '정리': 146,\n",
              " '결혼': 147,\n",
              " '라': 148,\n",
              " '또': 149,\n",
              " '제': 150,\n",
              " '아서': 151,\n",
              " '...': 152,\n",
              " '때문': 153,\n",
              " '날': 154,\n",
              " 'ㅂ시오': 155,\n",
              " '인데': 156,\n",
              " '서': 157,\n",
              " '와': 158,\n",
              " '달': 159,\n",
              " '짝사랑': 160,\n",
              " '중': 161,\n",
              " '같이': 162,\n",
              " '니': 163,\n",
              " '기에': 164,\n",
              " '오': 165,\n",
              " '녀': 166,\n",
              " '그녀': 167,\n",
              " '다면': 168,\n",
              " '중요': 169,\n",
              " '이랑': 170,\n",
              " '참': 171,\n",
              " '고백': 172,\n",
              " '후회': 173,\n",
              " '지금': 174,\n",
              " '될': 175,\n",
              " '해의': 176,\n",
              " '살펴보': 177,\n",
              " '행복': 178,\n",
              " '돼': 179,\n",
              " '싫': 180,\n",
              " '보다': 181,\n",
              " '면서': 182,\n",
              " '조금': 183,\n",
              " '왔': 184,\n",
              " '아직': 185,\n",
              " '좋아하': 186,\n",
              " '봐': 187,\n",
              " '나의': 188,\n",
              " '먼저': 189,\n",
              " '그런': 190,\n",
              " '준비': 191,\n",
              " '사귀': 192,\n",
              " '는지': 193,\n",
              " '봅니다': 194,\n",
              " '시작': 195,\n",
              " '돼요': 196,\n",
              " '혼자': 197,\n",
              " '고민': 198,\n",
              " '군요': 199,\n",
              " '바랄게요': 200,\n",
              " '감정': 201,\n",
              " '사': 202,\n",
              " '꿈': 203,\n",
              " '서로': 204,\n",
              " '인가': 205,\n",
              " '다고': 206,\n",
              " '못하': 207,\n",
              " '물': 208,\n",
              " '계속': 209,\n",
              " '합니다': 210,\n",
              " '힘든': 211,\n",
              " '줄': 212,\n",
              " '맞': 213,\n",
              " '선물': 214,\n",
              " '으며': 215,\n",
              " '술': 216,\n",
              " '하나': 217,\n",
              " '힘드': 218,\n",
              " '꼼짝': 219,\n",
              " '번': 220,\n",
              " '될까': 221,\n",
              " '까지': 222,\n",
              " '가능': 223,\n",
              " '만큼': 224,\n",
              " '진짜': 225,\n",
              " '나요': 226,\n",
              " '두': 227,\n",
              " '이야기': 228,\n",
              " '자': 229,\n",
              " '시': 230,\n",
              " '기분': 231,\n",
              " '기억': 232,\n",
              " '그럴': 233,\n",
              " '집': 234,\n",
              " '든': 235,\n",
              " '자꾸': 236,\n",
              " '해서': 237,\n",
              " '쉽': 238,\n",
              " '후': 239,\n",
              " '표현': 240,\n",
              " '걸까': 241,\n",
              " '듯': 242,\n",
              " '믿': 243,\n",
              " '너': 244,\n",
              " '데이트': 245,\n",
              " '여친': 246,\n",
              " '슬픔': 247,\n",
              " '자고': 248,\n",
              " '됐': 249,\n",
              " '인지': 250,\n",
              " '상처': 251,\n",
              " '신경': 252,\n",
              " '어떨까': 253,\n",
              " '도록': 254,\n",
              " '니까요': 255,\n",
              " '맘': 256,\n",
              " '쓰': 257,\n",
              " '라도': 258,\n",
              " '미련': 259,\n",
              " '걱정': 260,\n",
              " '눈': 261,\n",
              " '였': 262,\n",
              " '무슨': 263,\n",
              " '다가': 264,\n",
              " '카톡': 265,\n",
              " '째': 266,\n",
              " '볼까': 267,\n",
              " '남친': 268,\n",
              " '대화': 269,\n",
              " '쉬': 270,\n",
              " '입니다': 271,\n",
              " '이해': 272,\n",
              " '이상': 273,\n",
              " '만들': 274,\n",
              " '함께': 275,\n",
              " '바랍니다': 276,\n",
              " '어도': 277,\n",
              " '어떤': 278,\n",
              " '씩': 279,\n",
              " '놀': 280,\n",
              " '함': 281,\n",
              " ',': 282,\n",
              " '을까요': 283,\n",
              " '언제': 284,\n",
              " '어디': 285,\n",
              " '헤어진지': 286,\n",
              " '연인': 287,\n",
              " '하루': 288,\n",
              " '돈': 289,\n",
              " '관심': 290,\n",
              " '누구': 291,\n",
              " '헤어졌': 292,\n",
              " '이에': 293,\n",
              " '긴': 294,\n",
              " '아프': 295,\n",
              " '러': 296,\n",
              " '보내': 297,\n",
              " '썸남': 298,\n",
              " '음': 299,\n",
              " '셨': 300,\n",
              " '어렵': 301,\n",
              " '카나': 302,\n",
              " '여행': 303,\n",
              " '그냥': 304,\n",
              " '2': 305,\n",
              " '곳': 306,\n",
              " '된': 307,\n",
              " '몰라요': 308,\n",
              " '한두': 309,\n",
              " '속': 310,\n",
              " '새로운': 311,\n",
              " '이렇게': 312,\n",
              " '똑같': 313,\n",
              " '잡': 314,\n",
              " '앞': 315,\n",
              " '1': 316,\n",
              " '3': 317,\n",
              " '어떡': 318,\n",
              " '라는': 319,\n",
              " '는데요': 320,\n",
              " '니까': 321,\n",
              " '기다리': 322,\n",
              " '자주': 323,\n",
              " '내일': 324,\n",
              " '첫': 325,\n",
              " '마지막': 326,\n",
              " '부담': 327,\n",
              " '줘': 328,\n",
              " '만날': 329,\n",
              " '따라': 330,\n",
              " '다는': 331,\n",
              " '아침': 332,\n",
              " '이유': 333,\n",
              " '헤어진': 334,\n",
              " '궁금': 335,\n",
              " '바라': 336,\n",
              " '선택': 337,\n",
              " '결정': 338,\n",
              " '그렇게': 339,\n",
              " '추억': 340,\n",
              " '오래': 341,\n",
              " '려고': 342,\n",
              " '라면': 343,\n",
              " '별': 344,\n",
              " '나이': 345,\n",
              " '우리': 346,\n",
              " '이젠': 347,\n",
              " '결국': 348,\n",
              " '머리': 349,\n",
              " '뿐': 350,\n",
              " '가능성': 351,\n",
              " '비': 352,\n",
              " '대': 353,\n",
              " '노래': 354,\n",
              " '생각나': 355,\n",
              " '사이': 356,\n",
              " '텐데': 357,\n",
              " '공부': 358,\n",
              " '나가': 359,\n",
              " '올': 360,\n",
              " '님': 361,\n",
              " '처럼': 362,\n",
              " '노력': 363,\n",
              " '울': 364,\n",
              " '못가': 365,\n",
              " '워낙': 366,\n",
              " '입': 367,\n",
              " '꼭': 368,\n",
              " '덜': 369,\n",
              " '은데요': 370,\n",
              " '없었': 371,\n",
              " '젊은이': 372,\n",
              " '죽': 373,\n",
              " '잠': 374,\n",
              " '부터': 375,\n",
              " '순간': 376,\n",
              " '개월': 377,\n",
              " '도움': 378,\n",
              " '생겼': 379,\n",
              " '늦': 380,\n",
              " '대로': 381,\n",
              " '정도': 382,\n",
              " '상대': 383,\n",
              " '드': 384,\n",
              " '용기': 385,\n",
              " '이런': 386,\n",
              " '느낌': 387,\n",
              " '차': 388,\n",
              " '항상': 389,\n",
              " '변화': 390,\n",
              " '으니까요': 391,\n",
              " '건가': 392,\n",
              " '분': 393,\n",
              " '스럽': 394,\n",
              " '직접': 395,\n",
              " '문제': 396,\n",
              " '운동': 397,\n",
              " '인연': 398,\n",
              " '말로': 399,\n",
              " '만났': 400,\n",
              " '보이': 401,\n",
              " '인생': 402,\n",
              " '일까': 403,\n",
              " '거나': 404,\n",
              " '답답': 405,\n",
              " '잘못': 406,\n",
              " '의미': 407,\n",
              " '상황': 408,\n",
              " '반': 409,\n",
              " '관계': 410,\n",
              " '그만': 411,\n",
              " '질': 412,\n",
              " '남편': 413,\n",
              " '만남': 414,\n",
              " '어제': 415,\n",
              " '모든': 416,\n",
              " '괴로움': 417,\n",
              " '매일': 418,\n",
              " '가슴': 419,\n",
              " '기대': 420,\n",
              " '전화': 421,\n",
              " '다르': 422,\n",
              " '감': 423,\n",
              " '한가': 424,\n",
              " '큰': 425,\n",
              " '가지': 426,\n",
              " '자기': 427,\n",
              " '재회': 428,\n",
              " '붙잡': 429,\n",
              " '그게': 430,\n",
              " '모두': 431,\n",
              " '건지': 432,\n",
              " '볼': 433,\n",
              " '헤어짐': 434,\n",
              " '으세요': 435,\n",
              " '확인': 436,\n",
              " '나쁜': 437,\n",
              " '마시': 438,\n",
              " '힘': 439,\n",
              " '드세요': 440,\n",
              " '까': 441,\n",
              " '놓': 442,\n",
              " '진': 443,\n",
              " '세상': 444,\n",
              " '현실': 445,\n",
              " '아픔': 446,\n",
              " '찾': 447,\n",
              " '충분히': 448,\n",
              " '그렇': 449,\n",
              " '내고': 450,\n",
              " '찾아보': 451,\n",
              " '봤': 452,\n",
              " '제일': 453,\n",
              " '부모': 454,\n",
              " '인의': 455,\n",
              " '둘': 456,\n",
              " '가장': 457,\n",
              " '지내': 458,\n",
              " '진심': 459,\n",
              " '스스로': 460,\n",
              " '천천히': 461,\n",
              " '이나': 462,\n",
              " '어야': 463,\n",
              " '처음': 464,\n",
              " '별로': 465,\n",
              " '듣': 466,\n",
              " '짧': 467,\n",
              " '몇': 468,\n",
              " '판단': 469,\n",
              " '아도': 470,\n",
              " '없이': 471,\n",
              " '그래도': 472,\n",
              " '갖': 473,\n",
              " '얼굴': 474,\n",
              " '답': 475,\n",
              " '넘': 476,\n",
              " '다음': 477,\n",
              " '몸': 478,\n",
              " '편': 479,\n",
              " '상대방': 480,\n",
              " 'ㄴ다면': 481,\n",
              " '밥': 482,\n",
              " '다가가': 483,\n",
              " '갑자기': 484,\n",
              " '귀찮': 485,\n",
              " '실수': 486,\n",
              " '솔직': 487,\n",
              " '모습': 488,\n",
              " '마다': 489,\n",
              " '받아들이': 490,\n",
              " '행동': 491,\n",
              " '때때': 492,\n",
              " '협조': 493,\n",
              " '씹': 494,\n",
              " '아닌': 495,\n",
              " '사진': 496,\n",
              " '추천': 497,\n",
              " '후폭풍': 498,\n",
              " '갔': 499,\n",
              " '딱': 500,\n",
              " '아무': 501,\n",
              " '글': 502,\n",
              " '확실': 503,\n",
              " '을지': 504,\n",
              " '월과': 505,\n",
              " '건강': 506,\n",
              " '밤': 507,\n",
              " '줬': 508,\n",
              " '요즘': 509,\n",
              " '난': 510,\n",
              " '티': 511,\n",
              " '영화': 512,\n",
              " '호감': 513,\n",
              " '만이': 514,\n",
              " '화': 515,\n",
              " '다니': 516,\n",
              " '버리': 517,\n",
              " '5': 518,\n",
              " '빨리': 519,\n",
              " '습관': 520,\n",
              " '주말': 521,\n",
              " '났': 522,\n",
              " '보여': 523,\n",
              " '갈': 524,\n",
              " '눈물': 525,\n",
              " '웃': 526,\n",
              " '극복': 527,\n",
              " '대한': 528,\n",
              " '이루어지': 529,\n",
              " '드릴게요': 530,\n",
              " '그분': 531,\n",
              " '4': 532,\n",
              " '졌': 533,\n",
              " '만난': 534,\n",
              " '잊혀': 535,\n",
              " '익숙': 536,\n",
              " '톡': 537,\n",
              " '형': 538,\n",
              " '운명': 539,\n",
              " '나와': 540,\n",
              " '날씨': 541,\n",
              " '도와': 542,\n",
              " '6': 543,\n",
              " '차단': 544,\n",
              " '주기도': 545,\n",
              " '캐치': 546,\n",
              " '선': 547,\n",
              " '회사': 548,\n",
              " '낫': 549,\n",
              " '복잡': 550,\n",
              " '포기': 551,\n",
              " '새': 552,\n",
              " '편지': 553,\n",
              " '인정': 554,\n",
              " '란': 555,\n",
              " '바쁘': 556,\n",
              " '맛있': 557,\n",
              " '열심히': 558,\n",
              " '설레': 559,\n",
              " '준': 560,\n",
              " '차이': 561,\n",
              " '편하': 562,\n",
              " '얼른': 563,\n",
              " '가끔': 564,\n",
              " '챙겨': 565,\n",
              " '정신': 566,\n",
              " '뭘': 567,\n",
              " '소개팅': 568,\n",
              " '미안': 569,\n",
              " '언젠간': 570,\n",
              " '힘내': 571,\n",
              " '별후': 572,\n",
              " '자연': 573,\n",
              " '부유층': 574,\n",
              " '학교': 575,\n",
              " 'sns': 576,\n",
              " '어때': 577,\n",
              " '법': 578,\n",
              " '성격': 579,\n",
              " '확신': 580,\n",
              " '우울': 581,\n",
              " '세': 582,\n",
              " '변하': 583,\n",
              " '며': 584,\n",
              " '축하': 585,\n",
              " '충분': 586,\n",
              " '게임': 587,\n",
              " '생활': 588,\n",
              " '점점': 589,\n",
              " '원': 590,\n",
              " '성공': 591,\n",
              " '집착': 592,\n",
              " '얘기': 593,\n",
              " '삶': 594,\n",
              " '알아보': 595,\n",
              " '스트레스': 596,\n",
              " '관리': 597,\n",
              " '뭘까': 598,\n",
              " '냐': 599,\n",
              " '임': 600,\n",
              " '아야': 601,\n",
              " '말씀': 602,\n",
              " '동거': 603,\n",
              " '마주치': 604,\n",
              " '언제나': 605,\n",
              " '엄청': 606,\n",
              " '나오': 607,\n",
              " '아픈': 608,\n",
              " '쉬운': 609,\n",
              " '마련': 610,\n",
              " '약': 611,\n",
              " '소중': 612,\n",
              " '한다는': 613,\n",
              " '부분': 614,\n",
              " '그때': 615,\n",
              " '구불구불': 616,\n",
              " '무시': 617,\n",
              " '거짓말': 618,\n",
              " '상관': 619,\n",
              " '대해': 620,\n",
              " '예의': 621,\n",
              " '벌써': 622,\n",
              " '척': 623,\n",
              " '할지': 624,\n",
              " '약속': 625,\n",
              " '옆': 626,\n",
              " '읽': 627,\n",
              " '위로': 628,\n",
              " '적기': 629,\n",
              " '연습': 630,\n",
              " '봄': 631,\n",
              " '아닌데': 632,\n",
              " '기다려': 633,\n",
              " '짜증': 634,\n",
              " '보통': 635,\n",
              " '후련': 636,\n",
              " '문자': 637,\n",
              " '바람': 638,\n",
              " '한다고': 639,\n",
              " '점': 640,\n",
              " '카페': 641,\n",
              " '지났': 642,\n",
              " '크': 643,\n",
              " '깊': 644,\n",
              " '잠시': 645,\n",
              " '스러워': 646,\n",
              " '무엇': 647,\n",
              " '야가': 648,\n",
              " '해질': 649,\n",
              " '마요': 650,\n",
              " '진정': 651,\n",
              " '뭔지': 652,\n",
              " '한데': 653,\n",
              " '예쁘': 654,\n",
              " '바': 655,\n",
              " '존중': 656,\n",
              " '뒤': 657,\n",
              " '피곤': 658,\n",
              " '여': 659,\n",
              " '아무래도': 660,\n",
              " '그리고': 661,\n",
              " '려': 662,\n",
              " '위해': 663,\n",
              " '응원': 664,\n",
              " '로서': 665,\n",
              " '본인': 666,\n",
              " '그럼': 667,\n",
              " '풀': 668,\n",
              " '동안': 669,\n",
              " '미치': 670,\n",
              " '따뜻': 671,\n",
              " '떠나': 672,\n",
              " '잔': 673,\n",
              " '악몽': 674,\n",
              " '과정': 675,\n",
              " '는다면': 676,\n",
              " '주변': 677,\n",
              " '분간': 678,\n",
              " '잊어버리': 679,\n",
              " '취미': 680,\n",
              " '지나': 681,\n",
              " '귀': 682,\n",
              " '스러운': 683,\n",
              " '맨날': 684,\n",
              " '욕': 685,\n",
              " '어려워': 686,\n",
              " '얼마': 687,\n",
              " '커피': 688,\n",
              " '한지': 689,\n",
              " '접': 690,\n",
              " '주일': 691,\n",
              " '으니': 692,\n",
              " '어느': 693,\n",
              " '아닌지': 694,\n",
              " '적극': 695,\n",
              " '재밌': 696,\n",
              " '소리': 697,\n",
              " '생일': 698,\n",
              " '줄까': 699,\n",
              " '일어나': 700,\n",
              " '바로': 701,\n",
              " '끊': 702,\n",
              " '찍': 703,\n",
              " '질투': 704,\n",
              " '삭제': 705,\n",
              " '통보': 706,\n",
              " '슬픈': 707,\n",
              " '착각': 708,\n",
              " '원망': 709,\n",
              " '숨': 710,\n",
              " '흐르': 711,\n",
              " '정말로': 712,\n",
              " '전해': 713,\n",
              " '자체': 714,\n",
              " '나눠': 715,\n",
              " '아닐까요': 716,\n",
              " '버렸': 717,\n",
              " '아파': 718,\n",
              " '오빠': 719,\n",
              " '나중': 720,\n",
              " '능력': 721,\n",
              " '자존': 722,\n",
              " '아무것': 723,\n",
              " '돌아오': 724,\n",
              " '작': 725,\n",
              " '답장': 726,\n",
              " '잠깐': 727,\n",
              " '헷갈리': 728,\n",
              " '여전히': 729,\n",
              " '이성': 730,\n",
              " '자는': 731,\n",
              " '바보': 732,\n",
              " '경우': 733,\n",
              " '사친': 734,\n",
              " '생기': 735,\n",
              " '한다면': 736,\n",
              " '다를': 737,\n",
              " '달라지': 738,\n",
              " '해야지': 739,\n",
              " '재미': 740,\n",
              " '까먹': 741,\n",
              " '본': 742,\n",
              " '종교': 743,\n",
              " '스타일': 744,\n",
              " '구': 745,\n",
              " '여유': 746,\n",
              " '옷': 747,\n",
              " '돌아가': 748,\n",
              " '여기': 749,\n",
              " '인기': 750,\n",
              " '드리': 751,\n",
              " '보냈': 752,\n",
              " '흔들리': 753,\n",
              " '거절': 754,\n",
              " '생길': 755,\n",
              " '때쯤': 756,\n",
              " '가족': 757,\n",
              " '최고': 758,\n",
              " '어떻': 759,\n",
              " '멀': 760,\n",
              " '바뀌': 761,\n",
              " '부족': 762,\n",
              " '일찍': 763,\n",
              " '밖': 764,\n",
              " '원래': 765,\n",
              " '가져': 766,\n",
              " '엄마': 767,\n",
              " '려면': 768,\n",
              " '타이밍': 769,\n",
              " '곧': 770,\n",
              " '어쩔': 771,\n",
              " '프': 772,\n",
              " '두려워': 773,\n",
              " '사실': 774,\n",
              " '여러': 775,\n",
              " '장거리': 776,\n",
              " '어떠하': 777,\n",
              " '남의': 778,\n",
              " 'ㄴ가요': 779,\n",
              " '오자키': 780,\n",
              " '더라고요': 781,\n",
              " '갈까': 782,\n",
              " '겠지': 783,\n",
              " '시켜': 784,\n",
              " '기본': 785,\n",
              " '예쁜': 786,\n",
              " '번호': 787,\n",
              " '쌍': 788,\n",
              " '어색': 789,\n",
              " '단': 790,\n",
              " '드디어': 791,\n",
              " '손': 792,\n",
              " '핸드폰': 793,\n",
              " '상담': 794,\n",
              " '이러': 795,\n",
              " '장': 796,\n",
              " '엔': 797,\n",
              " '지나가': 798,\n",
              " '못한': 799,\n",
              " '그런가': 800,\n",
              " '알려': 801,\n",
              " '맞춰': 802,\n",
              " '넘겨받': 803,\n",
              " '건과': 804,\n",
              " '즐거운': 805,\n",
              " '도전': 806,\n",
              " '예민': 807,\n",
              " '칭찬': 808,\n",
              " '분위기': 809,\n",
              " '인사': 810,\n",
              " '안녕': 811,\n",
              " '기간': 812,\n",
              " '미리': 813,\n",
              " '깨': 814,\n",
              " '자유': 815,\n",
              " '집중': 816,\n",
              " '간': 817,\n",
              " '환승': 818,\n",
              " '짐': 819,\n",
              " '이기': 820,\n",
              " '과연': 821,\n",
              " '그대로': 822,\n",
              " '더라도': 823,\n",
              " '느끼': 824,\n",
              " '시원': 825,\n",
              " '오랫동안': 826,\n",
              " '그래요': 827,\n",
              " '서운': 828,\n",
              " '인은': 829,\n",
              " '수많': 830,\n",
              " '할게요': 831,\n",
              " '야지': 832,\n",
              " '그래': 833,\n",
              " '간다': 834,\n",
              " '완전': 835,\n",
              " '써': 836,\n",
              " '새벽': 837,\n",
              " '한잔': 838,\n",
              " '하늘': 839,\n",
              " '7': 840,\n",
              " '위한': 841,\n",
              " '문득': 842,\n",
              " '반복': 843,\n",
              " '쯤': 844,\n",
              " '쓰이': 845,\n",
              " '최선': 846,\n",
              " '배려': 847,\n",
              " '탈': 848,\n",
              " '금방': 849,\n",
              " '허전': 850,\n",
              " '거기': 851,\n",
              " '오세아니아': 852,\n",
              " '으로서': 853,\n",
              " '으나': 854,\n",
              " '다행': 855,\n",
              " '어쩌': 856,\n",
              " '한다': 857,\n",
              " '배우': 858,\n",
              " '더니': 859,\n",
              " '느껴': 860,\n",
              " '싸우': 861,\n",
              " '책': 862,\n",
              " '심해': 863,\n",
              " '사과': 864,\n",
              " '자리': 865,\n",
              " '멋진': 866,\n",
              " '식': 867,\n",
              " '10': 868,\n",
              " '는다는': 869,\n",
              " '조언': 870,\n",
              " '잠수': 871,\n",
              " '견디': 872,\n",
              " '됩니다': 873,\n",
              " '변덕': 874,\n",
              " '계산': 875,\n",
              " '걸로': 876,\n",
              " '대요': 877,\n",
              " '겨드랑이': 878,\n",
              " '현재': 879,\n",
              " '성행위': 880,\n",
              " '자책': 881,\n",
              " '괜히': 882,\n",
              " '커플': 883,\n",
              " '무': 884,\n",
              " '좋아해': 885,\n",
              " '뜻': 886,\n",
              " '피하': 887,\n",
              " '아무리': 888,\n",
              " '조심': 889,\n",
              " '냐고': 890,\n",
              " '지우': 891,\n",
              " '8': 892,\n",
              " '소식': 893,\n",
              " '빌': 894,\n",
              " '결심': 895,\n",
              " '어려운': 896,\n",
              " '오해': 897,\n",
              " '될까요': 898,\n",
              " '눈치': 899,\n",
              " '카': 900,\n",
              " '각기': 901,\n",
              " '사세요': 902,\n",
              " '주무세요': 903,\n",
              " '당황': 904,\n",
              " '전환': 905,\n",
              " '기회': 906,\n",
              " '짓': 907,\n",
              " '비싸': 908,\n",
              " '비밀': 909,\n",
              " '뭔가': 910,\n",
              " '소개': 911,\n",
              " '알바': 912,\n",
              " '찾아가': 913,\n",
              " '완벽': 914,\n",
              " '의심': 915,\n",
              " '우산': 916,\n",
              " '직장': 917,\n",
              " '흘렀': 918,\n",
              " '놈': 919,\n",
              " '떠난': 920,\n",
              " '땐': 921,\n",
              " '답니다': 922,\n",
              " '매력': 923,\n",
              " '좋아할': 924,\n",
              " '가요': 925,\n",
              " '인가요': 926,\n",
              " '의식적': 927,\n",
              " '의사': 928,\n",
              " '키우': 929,\n",
              " '무서워': 930,\n",
              " '위': 931,\n",
              " '쓰레기': 932,\n",
              " '떨려': 933,\n",
              " '제대로': 934,\n",
              " '벌': 935,\n",
              " '이루': 936,\n",
              " '밀': 937,\n",
              " '불안': 938,\n",
              " '끝내': 939,\n",
              " '꺼': 940,\n",
              " '비슷': 941,\n",
              " '결과': 942,\n",
              " '피해': 943,\n",
              " '플': 944,\n",
              " '이혼': 945,\n",
              " '제발': 946,\n",
              " '성': 947,\n",
              " '가사': 948,\n",
              " '완전히': 949,\n",
              " '휴': 950,\n",
              " '이거': 951,\n",
              " '라서': 952,\n",
              " '할수록': 953,\n",
              " '영원': 954,\n",
              " '썸녀': 955,\n",
              " '누군가': 956,\n",
              " '그것': 957,\n",
              " '때론': 958,\n",
              " '존재': 959,\n",
              " '사우스캐롤라이': 960,\n",
              " '고생': 961,\n",
              " '걔': 962,\n",
              " '폰': 963,\n",
              " '수록': 964,\n",
              " '내리': 965,\n",
              " '방학': 966,\n",
              " '군대': 967,\n",
              " '꽃': 968,\n",
              " '고치': 969,\n",
              " '화장': 970,\n",
              " '래': 971,\n",
              " '의지': 972,\n",
              " '힘든가': 973,\n",
              " '지쳤': 974,\n",
              " '도서관': 975,\n",
              " '실': 976,\n",
              " '연예인': 977,\n",
              " '으려고': 978,\n",
              " '일상': 979,\n",
              " '해결': 980,\n",
              " '으면서': 981,\n",
              " '나왔': 982,\n",
              " '귀엽': 983,\n",
              " '그리워': 984,\n",
              " '직전': 985,\n",
              " '헤어': 986,\n",
              " '이란': 987,\n",
              " '상태': 988,\n",
              " '고통': 989,\n",
              " '일지': 990,\n",
              " '고자': 991,\n",
              " '절대': 992,\n",
              " '실감': 993,\n",
              " '공감': 994,\n",
              " '은가': 995,\n",
              " '지속': 996,\n",
              " '쉐': 997,\n",
              " '그리워하': 998,\n",
              " '으시': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7812ee1a",
      "metadata": {
        "id": "7812ee1a"
      },
      "outputs": [],
      "source": [
        "def get_encoded_sentence(sentence, word_to_index):\n",
        "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bae5abd",
      "metadata": {
        "id": "5bae5abd"
      },
      "outputs": [],
      "source": [
        "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
        "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc00352b",
      "metadata": {
        "id": "cc00352b"
      },
      "outputs": [],
      "source": [
        "def vectorize(corpus, word_to_index):\n",
        "    data = []\n",
        "    for sen in corpus:\n",
        "        sen = get_encoded_sentence(sen, word_to_index)\n",
        "        data.append(sen)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ae3007",
      "metadata": {
        "id": "77ae3007"
      },
      "outputs": [],
      "source": [
        "que_train = vectorize(new_que_new_corpus, word_to_index)\n",
        "ans_train = vectorize(ans_corpus, word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5cbefa",
      "metadata": {
        "id": "5d5cbefa"
      },
      "outputs": [],
      "source": [
        "enc_train = keras.preprocessing.sequence.pad_sequences(que_train, padding='pre', maxlen=20)\n",
        "dec_train = keras.preprocessing.sequence.pad_sequences(ans_train, padding='pre', maxlen=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea05d5c",
      "metadata": {
        "id": "5ea05d5c",
        "outputId": "87799eb4-b13e-4e27-cdbb-55f64ff3ac4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0, 3208,  230, 3728,  116], dtype=int32)"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3900d4",
      "metadata": {
        "id": "4f3900d4",
        "outputId": "085196ff-b9a2-491a-db73-3aed267ae44c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "       288,   9, 149,   9,  41,  18,   4], dtype=int32)"
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dec_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aab510c",
      "metadata": {
        "id": "1aab510c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17625f8b",
      "metadata": {
        "id": "17625f8b"
      },
      "source": [
        " 1. 위 소스를 참고하여 타겟 데이터 전체에 '**\\<start**\\>' 토큰과 '**\\<end\\>**' 토큰을 추가\n",
        "\n",
        "- 챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것\n",
        "- 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있음\n",
        "\n",
        "  - 특수 토큰을 더함으로써 'ans_corpus' 또한 완성이 되었으니, 'que_corpus'와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 'enc_train'과 'dec_train'을 얻기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cd4d0d",
      "metadata": {
        "id": "e2cd4d0d"
      },
      "source": [
        "Step 6. 훈련하기\n",
        "\n",
        "---\n",
        "앞서 번역 모델을 훈련하며 정의한 ```Transformer``` 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! **가장 멋진 답변**과 **모델의 하이퍼파라미터**를 제출하시면 됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d00d1019",
      "metadata": {
        "id": "d00d1019"
      },
      "outputs": [],
      "source": [
        "#Positional Encoding\n",
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61dedca",
      "metadata": {
        "id": "d61dedca"
      },
      "outputs": [],
      "source": [
        "#마스크 생성\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0e7b6b",
      "metadata": {
        "id": "2b0e7b6b"
      },
      "outputs": [],
      "source": [
        "#Multi-head Attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
        "\n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        bsz = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "\n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "\n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask)\n",
        "\n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae79e75",
      "metadata": {
        "id": "aae79e75"
      },
      "outputs": [],
      "source": [
        "#Position-wise Feed Forward Network\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a1ed2b",
      "metadata": {
        "id": "80a1ed2b"
      },
      "outputs": [],
      "source": [
        "#Encoder Layer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, enc_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91ec3d3",
      "metadata": {
        "id": "e91ec3d3"
      },
      "outputs": [],
      "source": [
        "#Decoder Layer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.do(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d38d9e",
      "metadata": {
        "id": "c5d38d9e"
      },
      "outputs": [],
      "source": [
        "#Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "\n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "\n",
        "        return out, enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87845f7b",
      "metadata": {
        "id": "87845f7b"
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "\n",
        "\n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "\n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7cf219c",
      "metadata": {
        "id": "c7cf219c"
      },
      "outputs": [],
      "source": [
        "#Transformer 전체 모델 조립\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    pos_len,\n",
        "                    dropout=0.2,\n",
        "                    shared_fc=True,\n",
        "                    shared_emb=False):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        if shared_emb:\n",
        "            self.enc_emb = self.dec_emb = \\\n",
        "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        else:\n",
        "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.do = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared_fc = shared_fc\n",
        "\n",
        "        if shared_fc:\n",
        "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.do(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "\n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "\n",
        "        logits = self.fc(dec_out)\n",
        "\n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b55d466",
      "metadata": {
        "id": "4b55d466"
      },
      "outputs": [],
      "source": [
        "# 모델 인스턴스 생성\n",
        "VOCAB_SIZE = 30000\n",
        "\n",
        "transformer = Transformer(\n",
        "    n_layers=5,\n",
        "    d_model=512,\n",
        "    n_heads=16,\n",
        "    d_ff=2048,\n",
        "    src_vocab_size=VOCAB_SIZE,\n",
        "    tgt_vocab_size=VOCAB_SIZE,\n",
        "    pos_len=200,\n",
        "    dropout=0.3,\n",
        "    shared_fc=True,\n",
        "    shared_emb=True)\n",
        "\n",
        "d_model = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75652f7",
      "metadata": {
        "id": "b75652f7"
      },
      "outputs": [],
      "source": [
        "#Learning Rate Scheduler\n",
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096e2723",
      "metadata": {
        "id": "096e2723"
      },
      "outputs": [],
      "source": [
        "#Learing Rate & Optimizer\n",
        "learning_rate = LearningRateScheduler(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                        beta_1=0.9,\n",
        "                                        beta_2=0.98, \n",
        "                                        epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea90924a",
      "metadata": {
        "id": "ea90924a"
      },
      "outputs": [],
      "source": [
        "#Loss Function 정의\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70aab6eb",
      "metadata": {
        "id": "70aab6eb"
      },
      "outputs": [],
      "source": [
        "#Train Step 정의\n",
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    tgt_in = tgt[:, :-1]\n",
        "    gold = tgt[:, 1:]\n",
        "\n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5badf552",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c0df542e83e74004888e6d95a282ae4b",
            "784fa3be4fdf48ec81c416d5a3844aa2",
            "27c940b143f745b991f71a618b9c3e3a",
            "82d4caef178c4e0092b826ce4674dc43",
            "b3bbc5db44c04ddc97d2ec7845ce7a8c",
            "0f1f2d1069e14e2ba104307dbf45f6f7",
            "84faf540ab544ce2b02f5891802dea8d",
            "f9a3b43a073d4e6a9005ae3709bd7262",
            "25b23a94b8c349dfb7ce200a6ce47a98",
            "3bf7b9b6580341d0b50f2b0787b47cb5"
          ]
        },
        "id": "5badf552",
        "outputId": "656fa863-3602-4756-fc9b-fd7257783a93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_398/1860345807.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  t = tqdm_notebook(idx_list)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0df542e83e74004888e6d95a282ae4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "784fa3be4fdf48ec81c416d5a3844aa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27c940b143f745b991f71a618b9c3e3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d4caef178c4e0092b826ce4674dc43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3bbc5db44c04ddc97d2ec7845ce7a8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f1f2d1069e14e2ba104307dbf45f6f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84faf540ab544ce2b02f5891802dea8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9a3b43a073d4e6a9005ae3709bd7262",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25b23a94b8c349dfb7ce200a6ce47a98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bf7b9b6580341d0b50f2b0787b47cb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm_notebook \n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm_notebook(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                    dec_train[idx:idx+BATCH_SIZE],\n",
        "                    transformer,\n",
        "                    optimizer)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3882fa80",
      "metadata": {
        "id": "3882fa80"
      },
      "source": [
        "\n",
        "```\n",
        "# 예문\n",
        "1. 지루하다, 놀러가고 싶어.\n",
        "2. 오늘 일찍 일어났더니 피곤하다.\n",
        "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
        "4. 집에 있는다는 소리야.\n",
        "\n",
        "---\n",
        "\n",
        "# 제출\n",
        "\n",
        "Translations\n",
        "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
        "> 2. 맛난 거 드세요 . <end>\n",
        "> 3. 떨리 겠 죠 . <end>\n",
        "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
        "\n",
        "Hyperparameters\n",
        "> n_layers: 1\n",
        "> d_model: 368\n",
        "> n_heads: 8\n",
        "> d_ff: 1024\n",
        "> dropout: 0.2\n",
        "\n",
        "Training Parameters\n",
        "> Warmup Steps: 1000\n",
        "> Batch Size: 64\n",
        "> Epoch At: 10\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21674eea",
      "metadata": {
        "id": "21674eea"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence, model):\n",
        "    mecab = Mecab()\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    pieces = mecab.morphs(sentence)\n",
        "    \n",
        "    tokens = []\n",
        "    for sen in pieces:\n",
        "        sen= get_encoded_sentence(sen, word_to_index)\n",
        "        tokens.append(sen)\n",
        "    \n",
        "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
        "                                                        value=word_to_index[\"<pad>\"],\n",
        "                                                        padding='pre',\n",
        "                                                        maxlen=20)\n",
        "    \n",
        "    ids = []\n",
        "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
        "    for i in range(dec_train.shape[-1]):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "        generate_masks(_input, output)\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
        "        model(_input, \n",
        "              output,\n",
        "              enc_padding_mask,\n",
        "              combined_mask,\n",
        "              dec_padding_mask)\n",
        "\n",
        "        predicted_id = \\\n",
        "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
        "\n",
        "        if word_to_index[\"<end>\"] == predicted_id:\n",
        "            result = get_decoded_sentence(ids, index_to_word)\n",
        "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    result = get_decoded_sentence(ids, index_to_word)\n",
        "\n",
        "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "\n",
        "def translate(sentence, model):\n",
        "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "    evaluate(sentence, model)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713b8afa",
      "metadata": {
        "id": "713b8afa"
      },
      "outputs": [],
      "source": [
        "samples = [\"지루하다, 놀러가고 싶어.\",\n",
        "           \"오늘 일찍 일어났더니 피곤하다.\",\n",
        "           \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
        "           \"집에 있는다는 소리야.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4df3763",
      "metadata": {
        "id": "c4df3763",
        "outputId": "786b7c14-3163-4d00-aaed-a363809221f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample :  지루하다, 놀러가고 싶어.\n",
            "Translations :  하 는 건 위로 하 는 건 위로 하 는 건 위로 하 는 건 위로 하\n",
            "sample :  오늘 일찍 일어났더니 피곤하다.\n",
            "Translations :  하 지 마요 . 하 지 마요 . 하 게 좋 은 모두 하 세요 . 마요 .\n",
            "sample :  간만에 여자친구랑 데이트 하기로 했어.\n",
            "Translations :  중 했 다면 정말 하 는 사람 이 있 다면 정말 정말 정말 정말 정말 하 니까요 .\n",
            "sample :  집에 있는다는 소리야.\n",
            "Translations :  은 힘내 세요 . 하 고 하 고 좋 은 힘내 하 고 좋 겠 네요 .\n"
          ]
        }
      ],
      "source": [
        "predicts=[]\n",
        "for sample in samples:\n",
        "    predicts.append(translate(sample, transformer))\n",
        "    print('sample : ', sample)\n",
        "    print('Translations : ', translate(sample, transformer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0450ac7",
      "metadata": {
        "id": "c0450ac7"
      },
      "outputs": [],
      "source": [
        "translations=[\"잠깐 쉬 어도 돼요 . <end>\", \"맛난 거 드세요 . <end>\", \"떨리 겠 죠 . <end>\", \"좋 아 하 면 그럴 수 있 어요 . <end>\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc6bbc35",
      "metadata": {
        "id": "cc6bbc35"
      },
      "source": [
        "(1) BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efc6a9a",
      "metadata": {
        "id": "1efc6a9a"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4122d3d",
      "metadata": {
        "id": "d4122d3d",
        "outputId": "9fab79a1-0857-4af6-eed5-4c3a3714373e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score:  0\n",
            "BLEU Score:  0.010802314890908065\n",
            "BLEU Score:  0.010802314890908065\n",
            "BLEU Score:  0.015138514598766055\n"
          ]
        }
      ],
      "source": [
        "def calculate_bleu(translations, predicts, weights=[0.25, 0.25, 0.25, 0.25]):\n",
        "    for t, p in zip(translations, predicts):\n",
        "        print(\"BLEU Score: \", sentence_bleu([t.split()],\n",
        "                                            p.split(),\n",
        "                                            weights=weights,\n",
        "                                            smoothing_function=SmoothingFunction().method1))\n",
        "        \n",
        "calculate_bleu(translations, predicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75bf4da",
      "metadata": {
        "id": "e75bf4da"
      },
      "source": [
        "## 루브릭\n",
        "\n",
        "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
        "\n",
        "|평가문항|상세기준|\n",
        "|:-----|:------|\n",
        "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.|\n",
        "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.|\n",
        "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b20a47",
      "metadata": {
        "id": "46b20a47"
      },
      "source": [
        "# 회고\n",
        "\n",
        "이전의 익플 6에서 영화감성분석 리뷰가 떠오른던\n",
        "```ko.bin```을 다시 만나게 되었다.\n",
        "\n",
        "이게 파일 불러오기가 안되던 중 혜령님이 예전 노드에 겐심을 다운그레이드해야한다고 알려주셨다.!(바로 해결!)\n",
        "\n",
        "\n",
        "솔직히 nlp가 너무 어려워서 그 전 기수분들 노드를 참고안하면 진행하기가 너무 힘들었다. \n",
        "\n",
        "카피카피닌자의 능력을 부여받아 어찌저찌했지만👻<br>\n",
        "제대로 된 문장을 만들어내는것은 그 이후의 일.. 너무 어렵다😥\n",
        "\n",
        "패딩은 pre랑 post 둘 중 post가 좀 더 잘 나왔던 것 같다.\n",
        "큰 문제점은 하 다 를 를 같은 형태소가 단어와 단어사이를 메꿔야하는데 줄줄이 뒤를 이어나오는게.. 이건 데이터 정제할 때 고려를 했어야하는 사항이었던 것 같다.\n",
        "\n",
        "nlp는 언제나 이렇듯 완벽한 문장하나 내는게 이렇게 힘든 것인걸\n",
        "\n",
        "BLEU Score를 구하는건 하라고 해서 구하기는 했지만 제대로 된 사용을 모르겠다.<br>\n",
        "번역기를 만들었다면 제대로 나왔는지 비교를 할 수는 있지만 챗봇에서의 사용은 정해진 답이 있는 것도 아니라서 BLEU를 구하는게 맞는지 의문이 들었다.\n",
        "\n",
        "🌈🌈🌈그리고 갖은 에러와 난관에 부딪쳤을 때, 어김없이 도와준 혜령님께 슴슴한 감사를 드립니다\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "NLP[12]chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e75bf4da"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}