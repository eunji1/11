{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyB2OwJ4mTIn"
   },
   "source": [
    "## 10-5. 프로젝트: 더 멋진 번역기 만들기\n",
    "라이브러리 버전을 확인해 봅니다.\n",
    "\n",
    "---\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8sr1cseZH5p",
    "outputId": "b9b0b4a3-7a8d-4005-a26e-641281c5ca56"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smxLkvfnoxDO",
    "outputId": "a737e593-3907-4aa3-e215-bdb7793fd14e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0HEy-ADmXMs"
   },
   "source": [
    "Step 1. 데이터 다운로드 (클라우드 유저용)\n",
    "\n",
    "---\n",
    "아래 링크에서 ```korean-english-park.train.tar.gz``` 를 사용할 예정입니다. 다운로드할 필요는 없습니다.\n",
    "\n",
    "- [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n",
    "\n",
    "☁️클라우드 환경에서는 위 데이터를 미리 준비해 놓았으니 연결만 시켜줍시다. 우측 하단의 Cloud shell을 열어주세요.\n",
    "아래와 같이 공유 디렉토리에 저장된 데이터를 가리키는 심볼릭 링크를 생성해 주시면 됩니다.\n",
    "\n",
    "```\n",
    "$ ln -s ~/data ~/aiffel/transformer/data\n",
    "```\n",
    "\n",
    "Step 2. 데이터 정제 및 토큰화\n",
    "\n",
    "---\n",
    "\n",
    "1. ```set``` 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 ```cleaned_corpus``` 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ65Sq0IZH74"
   },
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    cleaned_corpus = list(set(zip(kor, eng)))\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hkKD6MLzw35",
    "outputId": "4752cb2b-df1d-4a95-eb79-1bfa6e8742f3"
   },
   "outputs": [],
   "source": [
    "print(cleaned_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17jUOHkEHx8W"
   },
   "source": [
    "한글(kor)에 영어가 포함된 문자는 제거해야할까?\n",
    "\n",
    "```\n",
    "US Iraq chief warns of long war이라크 미군부대장 장기전을 경고\n",
    "\n",
    "초당파적인 트위터보트리포트닷컴(TwitterVoteReport.com)에선 투표와 관련해 특별히 이름이 붙여진 트위터닷컴(Twitter.com)\n",
    "```\n",
    "\n",
    "영어와 한글 데이터를 분리해서 데이터 정제 하기로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl4PDETBmaKH"
   },
   "source": [
    "2. 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    ">- 모든 입력을 소문자로 변환합니다.<br>\n",
    ">- 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.<br>\n",
    ">- 문장부호 양옆에 공백을 추가합니다.<br>\n",
    ">- 문장 앞뒤의 불필요한 공백을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcpNTZ9AZIBe"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence_ko(sentence):\n",
    "    \n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"^\\s+|\\s+$\", \"\", sentence) #문자열의 앞, 뒤 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #문장부호 양 옆 공백 추가\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_en(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"^\\s+|\\s+$\", \"\", sentence) #문자열의 앞, 뒤 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #문장부호 양 옆 공백 추가\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWAaYSDgmb67"
   },
   "source": [
    "3. 한글 말뭉치 ```kor_corpus``` 와 영문 말뭉치 ```eng_corpus``` 를 각각 분리한 후, 정제하여 **토큰화**를 진행합니다! 토큰화에는 Sentencepiece를 활용하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 ```generate_tokenizer()``` 함수를 정의합니다. 최종적으로 ```ko_tokenizer``` 과 ```en_tokenizer``` 를 얻으세요. ```en_tokenizer```에는 ```set_encode_extra_options(\"bos:eos\")``` 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다.\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    ">- 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    ">- 학습 후 저장된 ```model``` 파일을 ```SentencePieceProcessor()``` 클래스에 ```Load()```한 후 반환합니다.\n",
    ">- 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "```<PAD>``` : 0 / ```<BOS>``` : 1 / ```<EOS>``` : 2 / ```<UNK>``` : 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ISObO-Lomfl7"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "\n",
    "# corpus를 받아 txt 파일로 저장\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/corpus_{lang}.txt'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm_{lang} --vocab_size={vocab_size}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm_{lang}.model')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2VN6EoVmpICZ"
   },
   "outputs": [],
   "source": [
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence_ko(k))\n",
    "    eng_corpus.append(preprocess_sentence_en(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lyrblDPHdbA",
    "outputId": "a92a001c-f210-426c-baab-f745da13ad89",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제러마이어 라이트 목사의 발언에 대해 격분하고 그런 광경을 접한 것에 대해 슬프다고 버락 오바마 의원이 일 현지시간 내셔널프레스클럽에서 밝혔다 . ', '일 병원에서 희생자들을 위문한 만모한 싱 인도 총리는 그들을 위로할 최상의 길은 양국간 관계 정상화 약속을 확고부동하게 이행하는 것 이라고 말했다 . ', '저항할 수 없이 ,  굉장히 ,  압도적으로']\n"
     ]
    }
   ],
   "source": [
    "print(kor_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bracNgiaHwb4",
    "outputId": "53da95a2-9609-40f2-c503-e19c14bf2fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sen .  barack obama said tuesday he was outraged by comments his former pastor ,  the rev .  jeremiah wright ,  made a day earlier at the national press club and saddened by the spectacle . ', 'indian prime minister manmohan singh ,  who also met victims at the hospital tuesday ,  said the best way to honor them was to remain steadfast in our commitment to normalized relations between our two countries . ', 'the un general assembly has voted overwhelmingly to create a new human rights organization for the world body ,  despite united states criticism . ']\n"
     ]
    }
   ],
   "source": [
    "print(eng_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "NK-rRewkpIKP",
    "outputId": "84b1eeb8-1515-4005-f43f-ef299ef09754"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/corpus_ko.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spm_ko --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/corpus_ko.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/corpus_ko.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78960 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4822906\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1167\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78960 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 142445 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78960\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 184940\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 184940 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=76019 obj=12.3652 num_tokens=357806 num_tokens/piece=4.7068\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=63795 obj=11.2616 num_tokens=359075 num_tokens/piece=5.62858\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=47846 obj=11.2567 num_tokens=374706 num_tokens/piece=7.8315\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=47832 obj=11.2246 num_tokens=375028 num_tokens/piece=7.84053\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35874 obj=11.3536 num_tokens=396517 num_tokens/piece=11.053\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35874 obj=11.32 num_tokens=396604 num_tokens/piece=11.0555\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26905 obj=11.4974 num_tokens=420453 num_tokens/piece=15.6273\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26905 obj=11.4582 num_tokens=420453 num_tokens/piece=15.6273\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=11.6028 num_tokens=436737 num_tokens/piece=19.8517\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=11.574 num_tokens=436739 num_tokens/piece=19.8518\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_ko.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm_ko.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/corpus_en.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spm_en --vocab_size=20000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/corpus_en.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/corpus_en.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78956 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10661485\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34535 obj=9.86221 num_tokens=83351 num_tokens/piece=2.41352\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25851 obj=8.00619 num_tokens=83809 num_tokens/piece=3.242\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21977 obj=7.92346 num_tokens=84668 num_tokens/piece=3.85257\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21848 obj=7.90465 num_tokens=84910 num_tokens/piece=3.8864\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm_en.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm_en.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvGe_5blmhgZ"
   },
   "source": [
    "4. 토크나이저를 활용해 **토큰의 길이가 50 이하**인 데이터를 선별하여 ```src_corpus``` 와 ```tgt_corpus``` 를 각각 구축하고, 텐서 ```enc_train``` 과 ```dec_train``` 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "X-GraCZzmf9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1139/3555803393.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx in tqdm_notebook(range(len(kor_corpus))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a9c1d0b7334315b5e63df32d81e7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다.\n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    src = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 50 and len(tgt) <= 50:\n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. 모델 설계\n",
    "\n",
    "---\n",
    "\n",
    "오늘 배운 내용을 활용해서 ```Transformer``` 모델을 설계해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제 1단계 - Positional Encoding\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제 2단계 - Multi-Head Attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "       \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Position-wise Feed-Forward Network\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#완성된 Transformer 구현\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate_padding_mask()** 는 Attention을 할 때에 PAD 토큰에도 Attention을 주는 것을 방지해 주는 역할을 합니다.\n",
    "\n",
    "Sequence-to-Sequence 모델에서 Loss에 대한 Masking을 해줄 때도 위와 같은 방법으로 진행합니다.\n",
    "\n",
    "한 배치의 데이터에서 PAD 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성합니다.\n",
    "\n",
    "\n",
    "첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태임을 알 수 있습니다.\n",
    "\n",
    "낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데... 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태입니다.\n",
    "\n",
    "자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹을 하는 것과 관련하여, 인과 관계를 가리는 것도 중요하지만 Decoder 역시 PAD 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmQXYMWJmj2j"
   },
   "source": [
    "\n",
    "\n",
    "Step 4. 훈련하기\n",
    "\n",
    "---\n",
    "앞서 필요한 것들을 모두 정의했기 때문에 우리는 훈련만 하면 됩니다! 아래 과정을 차근차근 따라가며 모델을 훈련하고, **예문에 대한 멋진 번역**을 제출하세요!\n",
    "\n",
    "1. 2 Layer를 가지는 ```Transformer```를 선언하세요.<br>(하이퍼파라미터는 자유롭게 조절합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt2QgIbEmnfM"
   },
   "source": [
    "2. 논문에서 사용한 것과 동일한 **Learning Rate Scheduler**를 선언하고, 이를 포함하는 **Adam Optimizer**를 선언하세요. (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZY37jSPmqnL"
   },
   "source": [
    "3. **Loss 함수를 정의**하세요.<br> Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, **Masking 되지 않은 입력의 개수로 Scaling**하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3Sz7G5NmtYa"
   },
   "source": [
    "4. ```train_step``` **함수**를 정의하세요.<br> **입력 데이터에 알맞은 Mask를 생성**하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "    \n",
    "    print(len(_input))\n",
    "    print(enc_train.shape[-1])\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        # InvalidArgumentError: In[0] mismatch In[1] shape: 50 vs. 1: [1,8,1,50] [1,8,1,64] 0 0 [Op:BatchMatMulV2]\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ypmw2zGEmmEX"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff = 2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSslUQ4omusB"
   },
   "source": [
    "5. **학습을 진행**합니다.<br> **매 Epoch 마다 제시된 예문에 대한 번역을 생성**하고, 멋진 번역이 생성되면 그때의 **하이퍼파라미터와 생성된 번역을 제출**하세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1139/234158154.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704e43a005284c51a9fc942cf5fb47a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 1. 오바마는 대통령이다.\n",
      "Predicted translation: the president is a day of the president .\n",
      "1\n",
      "50\n",
      "Input: 2. 시민들은 도시 속에 산다.\n",
      "Predicted translation: the government has been a second place in the city of the city .\n",
      "1\n",
      "50\n",
      "Input: 3. 커피는 필요 없다.\n",
      "Predicted translation: the government is not to be a smaller .\n",
      "1\n",
      "50\n",
      "Input: 4. 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the south korean people died in the city of the city , a police official said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17639e0aaf454420a724653bd03c2165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 1. 오바마는 대통령이다.\n",
      "Predicted translation: the year old is the first time .\n",
      "1\n",
      "50\n",
      "Input: 2. 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s city is the city of the city s city .\n",
      "1\n",
      "50\n",
      "Input: 3. 커피는 필요 없다.\n",
      "Predicted translation: the coffee is not a key .\n",
      "1\n",
      "50\n",
      "Input: 4. 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the government has been killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085a58d1c22447da878ecc2d6653ef87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 1. 오바마는 대통령이다.\n",
      "Predicted translation: the obama administration is the first person to the obama administration .\n",
      "1\n",
      "50\n",
      "Input: 2. 시민들은 도시 속에 산다.\n",
      "Predicted translation: . . . . . . . .\n",
      "1\n",
      "50\n",
      "Input: 3. 커피는 필요 없다.\n",
      "Predicted translation: . coffee\n",
      "1\n",
      "50\n",
      "Input: 4. 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was killed and people were wounded in the past two days .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9585cbd7ec03435b84cbb70198d21949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 1. 오바마는 대통령이다.\n",
      "Predicted translation: . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "50\n",
      "Input: 2. 시민들은 도시 속에 산다.\n",
      "Predicted translation: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "50\n",
      "Input: 3. 커피는 필요 없다.\n",
      "Predicted translation: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "1\n",
      "50\n",
      "Input: 4. 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: . . . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cdcba982924e748d88947d09fa53b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50\n",
      "Input: 1. 오바마는 대통령이다.\n",
      "Predicted translation: . president obama is the first president to be the first president .\n",
      "1\n",
      "50\n",
      "Input: 2. 시민들은 도시 속에 산다.\n",
      "Predicted translation: . . . . . . .\n",
      "1\n",
      "50\n",
      "Input: 3. 커피는 필요 없다.\n",
      "Predicted translation: . not eating the rest of the country\n",
      "1\n",
      "50\n",
      "Input: 4. 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll from the death toll was .\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "\n",
    "## from tqdm import tqdm_notebook \n",
    "import random\n",
    "from tqdm import tqdm_notebook  \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "examples = [\n",
    "            \"1. 오바마는 대통령이다.\",\n",
    "            \"2. 시민들은 도시 속에 산다.\",\n",
    "            \"3. 커피는 필요 없다.\",\n",
    "            \"4. 일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예문\n",
    "```\n",
    "1. 오바마는 대통령이다.\n",
    "2. 시민들은 도시 속에 산다.\n",
    "3. 커피는 필요 없다.\n",
    "4. 일곱 명의 사망자가 발생했다.\n",
    "```\n",
    "에폭 1\n",
    "```\n",
    "the president is a day of the president .\n",
    "the government has been a second place in the city of the city .\n",
    "the government is not to be a smaller .\n",
    "the south korean people died in the city of the city , a police official said .\n",
    "```\n",
    "에폭 2\n",
    "```\n",
    "the year old is the first time .\n",
    "the city s city is the city of the city s city .\n",
    "the coffee is not a key .\n",
    "the government has been killed .\n",
    "```\n",
    "에폭 3\n",
    "```\n",
    "the obama administration is the first person to the obama administration .\n",
    ". . . . . . .\n",
    "coffee\n",
    "the death toll was killed and people were wounded in the past two days .\n",
    "```\n",
    "에폭 4\n",
    "```\n",
    ". . . . . . . . . . . . . . . . . . .\n",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
    ". . . . . .\n",
    "```\n",
    "에폭 5\n",
    "```\n",
    "president obama is the first president to be the first president .\n",
    ". . . . . .\n",
    "not eating the rest of the country\n",
    "the death toll from the death toll was .\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "## 열받아!!!!!!\n",
    "👿😈👿👿👿👿👿👿👿👿👿👿👿👿\n",
    "\n",
    "```OSError: Not found: unknown field name \"수익금과\" in TrainerSpec.```<br>\n",
    "에러 이 에러 이망할 에러<br>\n",
    "**sentencepiece**.....<br>\n",
    "에러가 난 이유를 찾으려 해도 도저히 못찾았다. 구글을 온종일 뒤져봐도 비슷한건 안나왔기 때문이다.👿<br>\n",
    "그래서 라이브러리 문제인가해서 버전도 다운그레이드했지만 안되었다.👿<br>\n",
    "\n",
    "다른 사람 코드를 참고한 결과\n",
    "```\n",
    "# corpus를 받아 txt 파일로 저장\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/transfomer/result/corpus_{lang}.txt'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "```\n",
    "\n",
    "이부분이 없어서 에러가 났던 것 같다.<br>\n",
    "프로젝트를 진행할 때 대강의 방향을 알려주지만 이부분은 따로 알려주지 않아서<br>\n",
    "여전히 왜 저게 필요한건지<br>\n",
    "저장을 안하면 노드 진행을 못하는 건지<br>\n",
    "하\n",
    "\n",
    "\n",
    "어이없어\n",
    "\n",
    "❕ 그리고\n",
    "\n",
    "어림없이 또 에러등장\n",
    "\n",
    "```NameError: name 'tqdm_notebook' is not defined```<br>\n",
    "노드에 주어진 코드는 이거였지만<br>\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "따로 찾아봤더니 <br>\n",
    "버전이 바뀌어서 tqdm에서는 tqdm_notebook이라는 명령어를 사용하지 못한다고 한다.<br>\n",
    "그래서 위의 코드가 맞고 lms에서는 최신 버전이 아니라서 밑에 코드가 작동하는 것 같다\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXyVoO3CZIHu"
   },
   "source": [
    "\n",
    ">## **루브릭**\n",
    ">\n",
    ">|번호|평가문항|상세기준|\n",
    ">|:---:|---|---|\n",
    ">|1|번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.|데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.|\n",
    ">|2|Transformer 번역기 모델이 정상적으로 구동된다.|Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.|\n",
    ">|3|테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다.|"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "10.트렌스포머.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
